{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../DataSet/Tokyo house price.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# # 检测文件编码\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# with open('./DataSet/Tokyo_20131_20234.csv', 'rb') as f:\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#     result = chardet.detect(f.read())\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# 使用检测到的编码读取文件\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../DataSet/Tokyo house price.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcp932\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1706\u001b[0m     f,\n\u001b[1;32m   1707\u001b[0m     mode,\n\u001b[1;32m   1708\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1709\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1710\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1711\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1712\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1713\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1714\u001b[0m )\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    864\u001b[0m             handle,\n\u001b[1;32m    865\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    866\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    867\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    868\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    869\u001b[0m         )\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../DataSet/Tokyo house price.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# # 检测文件编码\n",
    "# with open('./DataSet/Tokyo_20131_20234.csv', 'rb') as f:\n",
    "#     result = chardet.detect(f.read())\n",
    "#     encoding = result['encoding']\n",
    "#     print(f\"Detected encoding: {encoding}\")\n",
    "\n",
    "# 使用检测到的编码读取文件\n",
    "df = pd.read_csv('../DataSet/Tokyo house price.csv', encoding=\"cp932\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 列名列表\n",
    "columns = ['種類', '価格情報区分', '地域', '市区町村コード', '都道府県名', '市区町村名', '地区名', '最寄駅：名称','最寄駅：距離（分）', '取引価格（総額）', '坪単価', '面積（㎡）', '取引価格（㎡単価）', '土地の形状', '間口','延床面積（㎡）', '建築年', '建物の構造', '用途', '今後の利用目的', '前面道路：方位', '前面道路：種類','前面道路：幅員（ｍ）', '都市計画', '建ぺい率（％）', '容積率（％）', '取引時期', '取引の事情等']\n",
    "\n",
    "# 打印每一列的唯一值和唯一值的数量\n",
    "for column in columns:\n",
    "    unique_values = df[column].unique()\n",
    "    unique_count = df[column].nunique()\n",
    "    print(f\"{column}: 有 {unique_count} 个唯一值。\")\n",
    "    print(f\"这些唯一值是: {unique_values}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 筛选出'取引時期'列中包含2018年至2023年的数据\n",
    "df['取引時期'] = df['取引時期'].astype(str)\n",
    "filtered_df = df[df['取引時期'].str.contains(r'202[0-3]|201[8-9]', regex=True)]\n",
    "filtered_df = filtered_df .query('種類 == \"宅地(土地と建物)\"')\n",
    "filtered_df = filtered_df .query('種類 == \"宅地(土地と建物)\"')\n",
    "filtered_df = filtered_df .query('今後の利用目的 == \"住宅\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = ['最寄駅：距離（分）', '取引価格（総額）', '面積（㎡）','建築年', '建物の構造','地区名',\"建ぺい率（％）\",\"容積率（％）\"]\n",
    "selected_df = df[selected_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_df = selected_df.dropna()\n",
    "#astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取建筑年份的年份部分并转换为整数\n",
    "selected_df['建築年'] = selected_df['建築年'].str.extract('(\\d+)')\n",
    "selected_df = selected_df.dropna()\n",
    "selected_df['建築年'] = selected_df['建築年'].astype(int)\n",
    "# selected_df['建築年'] = selected_df['建築年'].astype(int)\n",
    "# selected_df['建筑年限'] = 2024 - selected_df['建築年']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_df.to_csv('exported_data3.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "# 使用OneHotEncoder进行独热编码\n",
    "one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "encoded_features = one_hot_encoder.fit_transform(selected_df[['地区名']])\n",
    "\n",
    "# 获取独热编码后的特征名\n",
    "feature_names = one_hot_encoder.get_feature_names_out(['地区名'])\n",
    "\n",
    "# 将编码后的特征转换为DataFrame\n",
    "encoded_df = pd.DataFrame(encoded_features, columns=feature_names)\n",
    "\n",
    "# 合并原始数据和独热编码后的数据\n",
    "df_onehot = pd.concat([selected_df, encoded_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用encoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# 确保列为字符串类型\n",
    "for column in [ '建物の構造','地区名']:\n",
    "    selected_df[column] = selected_df[column].astype(str)\n",
    "\n",
    "# 创建LabelEncoder对象\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# 对每个object列进行标签编码\n",
    "for column in [ '建物の構造','地区名']:\n",
    "    selected_df[column] = label_encoder.fit_transform(selected_df[column])\n",
    "\n",
    "print(selected_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_df['最寄駅：距離（分）'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义映射关系\n",
    "distance_mapping = {\n",
    "    '30分～60分': 30,\n",
    "    '1H～1H30': 60,\n",
    "    '1H30～2H': 90,\n",
    "    '2H～': 120,\n",
    "    '2,000㎡以上': 120\n",
    "}\n",
    "\n",
    "# 将文本值映射成数字\n",
    "selected_df['最寄駅：距離（分）'] = selected_df['最寄駅：距離（分）'].map(lambda x: distance_mapping[x] if x in distance_mapping else int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义映射关系\n",
    "distance_mapping = {\n",
    "    '2,000㎡以上': 2000\n",
    "}\n",
    "\n",
    "# 将文本值映射成数字\n",
    "selected_df['面積（㎡）'] = selected_df['面積（㎡）'].map(lambda x: distance_mapping[x] if x in distance_mapping else int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_df['面積（㎡）'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个函数来去除离群值\n",
    "# 四分位距（IQR）方法是一种常用的检测离群值的方法。它利用数据的第一四分位数（Q1）和第三四分位数（Q3）来定义一个范围，任何超出该范围的值都被视为离群值。\n",
    "def remove_outliers(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    df_filtered = df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n",
    "    return df_filtered\n",
    "\n",
    "# 去除 '取引価格（総額）' 的离群值\n",
    "data_cleaned = remove_outliers(selected_df, '取引価格（総額）')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# 定义一个函数来去除离群值\n",
    "# Z-Score 方法是通过计算每个数据点与均值的标准差来检测离群值的。通常，Z-Score 大于 3 或小于 -3 的数据点被视为离群值。\n",
    "def remove_outliers_zscore(df, column):\n",
    "    mean = df[column].mean()\n",
    "    std = df[column].std()\n",
    "    z_scores = (df[column] - mean) / std\n",
    "    df_filtered = df[np.abs(z_scores) < 3]\n",
    "    return df_filtered\n",
    "\n",
    "# 去除 '取引価格（総額）' 的离群值\n",
    "data_cleaned = remove_outliers_zscore(data_cleaned, '取引価格（総額）')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个函数来去除离群值\n",
    "# Z-Score 方法是通过计算每个数据点与均值的标准差来检测离群值的。通常，Z-Score 大于 3 或小于 -3 的数据点被视为离群值。\n",
    "def remove_outliers_zscore(df, column):\n",
    "    mean = df[column].mean()\n",
    "    std = df[column].std()\n",
    "    z_scores = (df[column] - mean) / std\n",
    "    df_filtered = df[np.abs(z_scores) < 3]\n",
    "    return df_filtered\n",
    "\n",
    "# 去除 '取引価格（総額）' 的离群值\n",
    "data_cleaned = remove_outliers_zscore(data_cleaned, '面積（㎡）')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个函数来去除离群值\n",
    "# 四分位距（IQR）方法是一种常用的检测离群值的方法。它利用数据的第一四分位数（Q1）和第三四分位数（Q3）来定义一个范围，任何超出该范围的值都被视为离群值。\n",
    "def remove_outliers(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    df_filtered = df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n",
    "    return df_filtered\n",
    "\n",
    "# 去除 '取引価格（総額）' 的离群值\n",
    "data_cleaned = remove_outliers(selected_df, '面積（㎡）')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个函数来去除离群值\n",
    "# 四分位距（IQR）方法是一种常用的检测离群值的方法。它利用数据的第一四分位数（Q1）和第三四分位数（Q3）来定义一个范围，任何超出该范围的值都被视为离群值。\n",
    "def remove_outliers(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    df_filtered = df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n",
    "    return df_filtered\n",
    "\n",
    "# 去除 '取引価格（総額）' 的离群值\n",
    "data_cleaned = remove_outliers(selected_df, '建築年')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_cleaned' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m filtered_data \u001b[38;5;241m=\u001b[39m data_cleaned[(data_cleaned[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m取引価格（総額）\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m5000000\u001b[39m) \u001b[38;5;241m&\u001b[39m (data_cleaned[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m取引価格（総額）\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100000000\u001b[39m)]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_cleaned' is not defined"
     ]
    }
   ],
   "source": [
    "filtered_data = data_cleaned[(data_cleaned['取引価格（総額）'] > 5000000) & (data_cleaned['取引価格（総額）'] <= 100000000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data = filtered_data[filtered_data['建築年'] > 1968]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data = filtered_data[filtered_data['面積（㎡）'] < 200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaned = data_cleaned[data_cleaned['最寄駅：距離（分）'] < 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaned.to_csv('exported_data5.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3428/3428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 9ms/step - loss: 1.0568 - r2_score: -39.1756 - val_loss: 0.7647 - val_r2_score: -34.6842\n",
      "Epoch 2/50\n",
      "\u001b[1m3428/3428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 9ms/step - loss: 0.8072 - r2_score: -39.4750 - val_loss: 0.7177 - val_r2_score: -35.6996\n",
      "Epoch 3/50\n",
      "\u001b[1m1646/3428\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 9ms/step - loss: 0.7611 - r2_score: -40.3189"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from datetime import datetime\n",
    "import os\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "data_cleaned = pd.read_csv(\"./exported_data5.csv\")\n",
    "X = data_cleaned.drop('取引価格（総額）', axis=1)\n",
    "y = data_cleaned['取引価格（総額）']\n",
    "\n",
    "# 数据标准化处理\n",
    "scaler_X = StandardScaler()\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "\n",
    "scaler_y = StandardScaler()\n",
    "y_scaled = scaler_y.fit_transform(y.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "# 将数据划分为训练集和验证集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "# 创建 Sequential 模型\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "# 添加输入层和隐藏层\n",
    "model.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dropout(0.5))  # 添加 dropout 防止过拟合\n",
    "# 添加输入层和隐藏层\n",
    "model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dropout(0.5))  # 添加 dropout 防止过拟合\n",
    "\n",
    "# 添加更多隐藏层\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# 添加输出层，输出层不需要激活函数（默认是线性输出）\n",
    "model.add(Dense(1, activation='linear'))  # 输出层只有一个神经元，用于预测\n",
    "\n",
    "optimizer = Adam(learning_rate=0.0001)\n",
    "\n",
    "# 自定义 R² 评估函数\n",
    "def r2_score(y_true, y_pred):\n",
    "    ss_res = K.sum(K.square(y_true - y_pred)) \n",
    "    ss_tot = K.sum(K.square(y_true - K.mean(y_true))) \n",
    "    return 1 - ss_res / (ss_tot + K.epsilon())\n",
    "\n",
    "# 编译模型，使用 MSE 损失函数\n",
    "model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=[r2_score])\n",
    "\n",
    "# 生成当前时间的字符串\n",
    "current_time = datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "\n",
    "# 创建日志目录名称\n",
    "log_dir = os.path.join('logs', current_time)\n",
    "\n",
    "# TensorBoard 回调函数\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# 训练模型batch_size=32,\n",
    "history = model.fit(X_train, y_train, epochs=50, validation_data=(X_test, y_test), callbacks=[tensorboard_callback, early_stopping_callback])\n",
    "\n",
    "# 模型评估\n",
    "mse = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f'Validation MSE: {mse:.4f}')\n",
    "\n",
    "# 预测验证集\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 反标准化预测结果\n",
    "y_pred_original = scaler_y.inverse_transform(y_pred.flatten().reshape(-1, 1)).flatten()\n",
    "y_val_original = scaler_y.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
    "\n",
    "# 计算评价指标\n",
    "mse = mean_squared_error(y_val_original, y_pred_original)\n",
    "mae = mean_absolute_error(y_val_original, y_pred_original)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_val_original, y_pred_original)\n",
    "mape = np.mean(np.abs((y_val_original - y_pred_original) / y_val_original)) * 100\n",
    "\n",
    "print(f'MSE: {mse:.4f}')\n",
    "print(f'MAE: {mae:.4f}')\n",
    "print(f'RMSE: {rmse:.4f}')\n",
    "print(f'R2: {r2:.4f}')\n",
    "print(f'MAPE: {mape:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from datetime import datetime\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import os\n",
    "data_cleaned = pd.read_csv(\"./exported_data5.csv\")\n",
    "\n",
    "# 将价格转换为类别标签\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(data_cleaned['取引価格（総額）'])\n",
    "\n",
    "# 数据标准化处理\n",
    "X = data_cleaned.drop('取引価格（総額）', axis=1)\n",
    "y = y_encoded\n",
    "\n",
    "scaler_X = StandardScaler()\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "\n",
    "# 将数据划分为训练集和验证集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # 创建 Sequential 模型\n",
    "# model = Sequential()\n",
    "# 构建模型\n",
    "model = Sequential([\n",
    "    Dense(128, input_dim=X_train.shape[1], activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(len(np.unique(y)), activation='softmax')# 添加输出层，使用 softmax 激活函数\n",
    "])\n",
    "\n",
    "# # 添加输入层和隐藏层\n",
    "# model.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))\n",
    "# model.add(Dropout(0.5))  # 添加 dropout 防止过拟合\n",
    "\n",
    "# # 添加更多隐藏层\n",
    "# model.add(Dense(64, activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "# 编译模型，使用交叉熵损失函数\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# 生成当前时间的字符串\n",
    "current_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# 创建日志目录名称\n",
    "log_dir = os.path.join('logs', current_time)\n",
    "\n",
    "# TensorBoard 回调函数\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# 训练模型\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test), callbacks=[tensorboard_callback, early_stopping_callback])\n",
    "\n",
    "# 模型评估\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f'Validation Loss: {loss:.4f}, Validation Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# 预测验证集\n",
    "y_pred_prob = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "# 计算准确率\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
